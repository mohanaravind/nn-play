<!--
TODO: 
1. Excel style input editor
-->

<link rel="import" href="../bower_components/polymer/polymer-element.html">
<link rel="import" href="shared-styles.html">

<dom-module id="my-view1">
  <template>
    <style include="shared-styles">
       :host {
        display: block;

        padding: 10px;
      }

      .container {
        display: flex;
        justify-content: center;
      }

      .layer {
        display: flex;
        flex-direction: column;
        justify-content: center;
      }

      .neuron {
        border-radius: 18px;
      }

      .inp {
        background-color: #f4cccc;
      }

      .hidden {
        background-color: #c9d9f8;
      }

      .out {
        background-color: #d8e9d3;
      }
    </style>

    <div class="card">
      <h1>Hyperparameters</h1>
      <div>

        <div class="hyperparameters-row">
          <labe>Layers</label>
            <input id="layers" type="text" value="3x4x1" />
        </div>
        <div class="hyperparameters-row">
          <labe>Learning Rate</label>
            <input id="learningrate" type="number" value="0.01" />
        </div>
        <div class="hyperparameters-row">
          <labe>Epochs</label>
            <input id="epochs" type="number" value="1" />
        </div>
        <div class="hyperparameters-row">
          <labe>Regularization</label>
            <input id="regularization" type="number" value="0.1" />
        </div>
        <div class="hyperparameters-row">
          <labe>Batchsize</label>
            <input id="batchsize" type="number" value="120" />
        </div>
        <div class="hyperparameters-row">
          <labe>Activation</label>
            <select id="activation">
            <option value="sigmoid">Sigmoid Function</option>
          </select>
        </div>


        <button on-click="_update">Update</button>

        <button on-click="_prepData">Prepare Data</button>

        <button on-click="_train">Train</button>
      </div>
    </div>

    <div class="card container">
      <div class="card layer inp">
        <template is="dom-repeat" items="[[network.inputs]]">
          <div class="card neuron">

          </div>
        </template>
      </div>

      <template is="dom-repeat" items="[[network.hiddenLayers]]">
        <div class="card layer hidden">
          <template is="dom-repeat" items="[[item]]">
            <div class="card neuron">

            </div>
          </template>
        </div>
      </template>

      <div class="card layer out">
        <template is="dom-repeat" items="[[network.outputs]]">
          <div class="card neuron">

          </div>
        </template>
      </div>
    </div>
  </template>
  <script src="../bower_components/numjs/dist/numjs.min.js"></script>
  <script>
    //Activations
    class Activation {
      // https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions
      static sigmoid(x) {
        // 1/(1 + e^-x)        
        let res = x.exp();
        res = res.pow(-1);
        res = res.add(1);
        res = res.pow(-1);

        return res;
      }

      static sigmoid_prime(x) {
        // x = sigmoid(x); return x * (1-x)
        const xSquared = x.pow(2);          
        return x.subtract(xSquared);
      }

      static relu(x) {
        //TODO:
        // http://kawahara.ca/what-is-the-derivative-of-relu/
      }

      static relu_prime(x) {
        //TODO:
      }
    }


    //Samples
    class SampleData {
      //TODO: https://archive.ics.uci.edu/ml/datasets.html
      static binarySample1() {
        const inputs = [
          [0, 0, 1],
          [1, 1, 1],
          [1, 0, 1],
          [0, 1, 1]
        ];

        const labels = [
          [0],
          [1],
          [1],
          [0]
        ];

        //Number of training sample
        const m = inputs.length;

        return {
          inputs,
          labels,
          m
        };
      }

      static sinCos() {        
        //  y = sin(x) + cos(x)
        const inputs = [];
        const labels = [];

        for (let idx = 1; idx < 5; idx++) {	
          const x1 = Math.sin(idx);
          const x2 = Math.cos(idx);

          inputs.push([x1, x2]);
          labels.push(x1 + x2 > 0 ? [1, 0] : [0, 1]);
        }

        //Number of training sample
        const m = inputs.length;
        
        return {
          inputs,
          labels,
          m
        };
      }
    }

    //Helper
    class Log {
      static success (message) {
        console.log(`%c ${message}! `, 'background:#8BC34A; color:black;border-radius:2px');
      }

      static info (message) {
        console.log(`%c ${message}! `, 'background:#CDDC39; color:black;border-radius:2px');
      }

    }


    class NeuralNetwork {
      constructor (info) {
        this.info = info;
      }

      //TODO: Any common utility and helper methods
    }

    class FullyConnected extends NeuralNetwork {

      constructor (info) {
        super(info);

        //Initialize the weights
        this._initializeWeights();
      }

      _initializeWeights () {              
        const arch = this.info.architecture;

        let weights = [];        
        let weights_delta = [];

        for (let idx = 0; idx + 1 < arch.length; idx++) {
          weights.push(nj.random([arch[idx], arch[idx + 1]]));
          weights_delta.push(nj.zeros(weights[idx].shape));
        }

        //Preserve the details in the network
        this.weights = weights;
        this.weights_delta = weights_delta;        
      }

      setData (data) {
        this.data = data;        
      }

      forward(input) {
        //Initialize
        const layers = [];
        input = nj.array(input);

        this.weights.forEach((weight, idx) => {
          const feedIn = nj.dot(input, weight);
          const feedOut = Activation.sigmoid(feedIn);

          //Preserve
          layers.push({input, weight, feedIn, feedOut });

          //Set this as input for the next layer
          input = feedOut;              
        });

        //Preserve
        this.layers = layers;

        //Return the final output
        return input;
      }

      backward(error) {
        const weights_delta = this.weights_delta;
        const layers = this.layers;

        //Go through the layers in reverse order
        for (let idx = layers.length - 1; idx > -1; idx--) {        
          const layer = layers[idx];

          //Calculate the gradient
          let gradient = Activation.sigmoid_prime(layer.feedOut).multiply(error);
          //Backprop the error
          error = nj.dot(layer.weight, gradient);

          //Reshape and compute the effect on the weight              
          let A = nj.array([layer.input.tolist()]);
          let B = nj.array([gradient.tolist()])
          let weight_gradient = A.T.dot(B);

          //Accumulate the weight gradient (Sum of gradients of all inputs)
          weights_delta[idx] = weights_delta[idx].add(weight_gradient);
        }
      }

      learn() {
        //Weights accumulator
        const weights_delta = this.weights_delta;
        const m = this.data.m;

        //Make the correction to the weights
        this.weights = this.weights.map((weight, idx) => {
          //Get the change that needs to be done to the weight
          const delta = weights_delta[idx].multiply(this.info.learningrate).divide(m);
          //Clear the weight delta from the accumulator so to be ready for next epoch
          weights_delta[idx] = nj.zeros(weight.shape);

          //Adjust the weight
          return weight.add(delta);
        });
      }

    }

    class MyView1 extends Polymer.Element {
      constructor() {
        super();
      }

      static get is() { return 'my-view1'; }

      ready() {
        super.ready();

        this._update();
      }

      static get properties() {
        return {
          neuralNetwork: {
            type: NeuralNetwork
          }
        };
      }

      //Update the network info with all hyperparameters and layers information
      _update() {
        const arch = this.$.layers.value.split('x').map(layer => Number(layer));

        const info = {
          architecture: arch.slice(),
          inputsCount: arch.shift(),
          outputsCount: arch.pop(),
          hiddenLayers: arch.map(count => new Array(count)),
          learningrate: parseFloat(this.$.learningrate.value),
          epochs: this.$.epochs.value,
          activation: this.$.activation.options[this.$.activation.selectedIndex].value
        };

        //TODO: Avoid this and use only one entity (Neural Network)
        //For the UI
        this.network = {
          inputs: Array(info.inputsCount),
          outputs: Array(info.outputsCount),
          hiddenLayers: info.hiddenLayers
        };

        //Instantiate the network
        //TODO: Make this dynamic based on the selected type of network
        this.neuralNetwork = new FullyConnected(info);         
      }

      _prepData() {
        //TODO: Use dropdown to dynamically allow selection of sample data
        //TODO: Update the layers input & output count and do the _update based on sample data

        Log.info('Data preparation started...');
        
        //Get sample data
        const data = SampleData.binarySample1();
        // const data =SampleData.sinCos();
        
        //Add it to the network info
        this.neuralNetwork.setData(data);

        Log.success('Data preparation completed!');
      }

      /*
        x
        Before weight multiplication
        
        feedIn
        After weight multiplication but before activation
        
        feedOut
        After activation
      */
      _train() {
        Log.info('Training started...');

        //Get the network        
        const nn = this.neuralNetwork;

        //Get necessary info about the neural network
        const epochs = nn.info.epochs;
        const {inputs, labels} = nn.data;        
        
        for (let epoch = 0; epoch < epochs; epoch++) {
          //Loop through all the inputs
          inputs.forEach( (input, idx) => {          
            //Do forward propagation
            let output = nn.forward(input);

            //Get the labels
            let y = nj.array(labels[idx]);
            //Compute the error
            let error = y.subtract(output);

            //Do backward propagation
            nn.backward(error);
          });

          //Adjust the weights by learning from the mistakes
          nn.learn();
        }

        Log.success('Training completed!');
      }
    }

    window.customElements.define(MyView1.is, MyView1);
  </script>
</dom-module>